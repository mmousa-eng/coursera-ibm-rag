{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Documents Using LangChain for Different Sources \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **20** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you are working as a data scientist at a consulting firm, and you've been tasked with analyzing documents from multiple clients. Each client provides their data in different formats: some in PDFs, others in Word documents, CSV files, or even HTML webpages. Manually loading and parsing each document type is not only time-consuming but also prone to errors. Your goal is to streamline this process, making it efficient and error-free.\n",
    "\n",
    "To achieve this, you'll use LangChain’s powerful document loaders. These loaders allow you to read and convert various file formats into a unified document structure that can be easily processed. For example, you'll load client policy documents from text files, financial reports from PDFs, marketing strategies from Word documents, and product reviews from JSON files. By the end of this lab, you will have a robust pipeline that can handle any new file formats clients might send, saving you valuable time and effort.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Hvf-jk8b5Fs-E_E4AJyEow/loader.png\" width=\"50%\" alt=\"indexing\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will explore how to use various loaders provided by LangChain to load and process data from different file formats. These loaders simplify the task of reading and converting files into a document format that can be processed downstream. By the end of this lab, you will be able to efficiently load text, PDF, Markdown, JSON, CSV, DOCX, and other file formats into a unified format, allowing for seamless data analysis and manipulation for LLM applications.\n",
    "\n",
    "(Note: In this lab, we just introduced several commonly used file format loaders. LangChain provides more document loaders for various document formats [here](https://python.langchain.com/v0.2/docs/integrations/document_loaders/).)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n",
    "            <li><a href=\"#Importing-Required-Libraries\">Importing required libraries</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Load-from-TXT-files\">Load from TXT files</a></li>\n",
    "    <li><a href=\"#Load-from-PDF-files\">Load from PDF files</a></li>\n",
    "    <li><a href=\"#Load-from-Markdown-files\">Load from Markdown files</a></li>\n",
    "    <li><a href=\"#Load-from-JSON-files\">Load from JSON files</a></li>\n",
    "    <li><a href=\"#Load-from-CSV-files\">Load from CSV files</a></li>\n",
    "    <li><a href=\"#Load-from-URL/Website-files\">Load from URL/Webpage files</a></li>\n",
    "    <li><a href=\"#Load-from-WORD-files\">Load from WORD files</a></li>\n",
    "    <li><a href=\"#Load-from-Unstructured-Files\">Load from Unstructured Files</a></li>\n",
    "</ol>\n",
    "\n",
    "<a href=\"#Exercises\">Exercises</a>\n",
    "<ol>\n",
    "    <li><a href=\"#Exercise-1---Try-to-use-other-PDF-loaders\">Exercise 1 - Try to use other PDF loaders</a></li>\n",
    "    <li><a href=\"#Exercise-2---Load-from-Arxiv\">Exercise 2 - Load from Arxiv</a></li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    " - Understand how to use `TextLoader` to load text files.\n",
    " - Learn how to load PDFs using `PyPDFLoader` and `PyMuPDFLoader`.\n",
    " - Use `UnstructuredMarkdownLoader` to load Markdown files.\n",
    " - Load JSON files with `JSONLoader` using jq schemas.\n",
    " - Process CSV files with `CSVLoader` and `UnstructuredCSVLoader`.\n",
    " - Load Webpage content using `WebBaseLoader`.\n",
    " - Load Word documents using `Docx2txtLoader`.\n",
    " - Utilize `UnstructuredFileLoader` for various file types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing required libraries\n",
    "\n",
    "The following required libraries are __not__ preinstalled in the Skills Network Labs environment. __You must run the following cell__ to install them:\n",
    "\n",
    "**Note:** We are pinning the version here to specify the version. It's recommended that you do this as well. Even if the library is updated in the future, the installed library could still support this lab work.\n",
    "\n",
    "This might take approximately 1 minute. \n",
    "\n",
    "As we use `%%capture` to capture the installation, you won't see the output process. But after the installation completes, you will see a number beside the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#After executing the cell,please RESTART the kernel and run all the cells.\n",
    "!pip install --user \"langchain-community==0.2.1\"\n",
    "!pip install --user \"pypdf==4.2.0\"\n",
    "!pip install --user \"PyMuPDF==1.24.5\"\n",
    "!pip install --user \"unstructured==0.14.8\"\n",
    "!pip install --user \"markdown==3.6\"\n",
    "!pip install --user  \"jq==1.7.0\"\n",
    "!pip install --user \"pandas==2.2.2\"\n",
    "!pip install --user \"docx2txt==0.8\"\n",
    "!pip install --user \"requests==2.32.3\"\n",
    "!pip install --user \"beautifulsoup4==4.12.3\"\n",
    "!pip install --user \"nltk==3.8.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you install the libraries, restart your kernel. You can do that by clicking the **Restart the kernel** icon.\n",
    "\n",
    "<p style=\"text-align:left\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/1_Bd_EvpEzLH9BbxRXXUGQ/screenshot-to-replace.png\" width=\"50%\"/>\n",
    "    </a>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "_We recommend you import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also use this section to suppress warnings generated by your code:\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pprint import pprint\n",
    "import json\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.document_loaders.csv_loader import UnstructuredCSVLoader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from TXT files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TextLoader` is a tool designed to load textual data from various sources.\n",
    "\n",
    "It is the simplest loader, reading a file as text and placing all the content into a single document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have prepared a .txt file for you to load. First, we need to download it from a remote source.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have prepared a .txt file for you to load. First, we need to download it from a remote source.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-22 16:38:33--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Ec5f3KYU1CpbKRp1whFLZw/new-Policies.txt\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6363 (6.2K) [text/plain]\n",
      "Saving to: ‘new-Policies.txt’\n",
      "\n",
      "new-Policies.txt    100%[===================>]   6.21K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-05-22 16:38:33 (924 MB/s) - ‘new-Policies.txt’ saved [6363/6363]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Ec5f3KYU1CpbKRp1whFLZw/new-Policies.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the `TextLoader` class to load the file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x7f474e210f50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = TextLoader(\"new-Policies.txt\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the `load` method to load the data as documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's present the entire data (document) here.\n",
    "\n",
    "This is a `document` object that includes `page_content` and `metadata` attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'new-Policies.txt'}, page_content=\"1. Code of Conduct\\n\\nOur Code of Conduct establishes the core values and ethical standards that all members of our organization must adhere to. We are committed to fostering a workplace characterized by integrity, respect, and accountability.\\n\\nIntegrity: We commit to the highest ethical standards by being honest and transparent in all our dealings, whether with colleagues, clients, or the community. We protect sensitive information and avoid conflicts of interest.\\n\\nRespect: We value diversity and every individual's contribution. Discrimination, harassment, or any form of disrespect is not tolerated. We promote an inclusive environment where differences are respected, and everyone is treated with dignity.\\n\\nAccountability: We are responsible for our actions and decisions, complying with all relevant laws and regulations. We aim for continuous improvement and report any breaches of this code, supporting investigations into such matters.\\n\\nSafety: We prioritize the safety of our employees, clients, and the community. We encourage a culture of safety, including reporting any unsafe practices or conditions.\\n\\nEnvironmental Responsibility: We strive to reduce our environmental impact and promote sustainable practices.\\n\\nThis Code of Conduct is the cornerstone of our organizational culture. We expect every employee to uphold these principles and act as role models, ensuring our reputation for ethical conduct, integrity, and social responsibility.\\n\\n2. Recruitment Policy\\n\\nOur Recruitment Policy is dedicated to attracting, selecting, and integrating the most qualified and diverse candidates into our organization. The success of our company depends on the talent, skills, and commitment of our employees.\\n\\nEqual Opportunity: We are an equal opportunity employer and do not discriminate based on race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or any other protected status. We actively support diversity and inclusion.\\n\\nTransparency: We maintain a transparent recruitment process. Job vacancies are advertised both internally and externally when appropriate. Job descriptions and requirements are clear and accurately reflect the role.\\n\\nSelection Criteria: We base our selection on qualifications, experience, and skills relevant to the role. Our interviews and assessments are objective, and decisions are made impartially.\\n\\nData Privacy: We are dedicated to protecting candidates' personal information and comply with all applicable data protection laws.\\n\\nFeedback: Candidates receive timely and constructive feedback on their applications and interview performance.\\n\\nOnboarding: New hires receive thorough onboarding to help them integrate effectively, including an overview of our culture, policies, and expectations.\\n\\nEmployee Referrals: We welcome employee referrals as they help build a strong and engaged team.\\n\\nThis policy lays the foundation for a diverse, inclusive, and talented workforce. It ensures that we hire candidates who align with our values and contribute to our success. We regularly review and update this policy to incorporate best practices in recruitment.\\n\\n\\n3. Internet and Email Policy\\n\\nOur Internet and Email Policy ensures the responsible and secure use of these tools within our organization, recognizing their importance in daily operations and the need for compliance with security, productivity, and legal standards.\\n\\nAcceptable Use: Company-provided internet and email are primarily for job-related tasks. Limited personal use is permitted during non-work hours as long as it does not interfere with work duties.\\n\\nSecurity: Protect your login credentials and avoid sharing passwords. Be cautious with email attachments and links from unknown sources, and promptly report any unusual online activity or potential security threats.\\n\\nConfidentiality: Use email for confidential information, trade secrets, and sensitive customer data only with encryption. Be careful when discussing company matters on public platforms or social media.\\n\\nHarassment and Inappropriate Content: Internet and email must not be used for harassment, discrimination, or the distribution of offensive content. Always communicate respectfully and sensitively online.\\n\\nCompliance: Adhere to all relevant laws and regulations concerning internet and email use, including copyright and data protection laws.\\n\\nMonitoring: The company reserves the right to monitor internet and email usage for security and compliance purposes.\\n\\nConsequences: Violations of this policy may lead to disciplinary action, including potential termination.\\n\\nThis policy promotes the safe and responsible use of digital communication tools in line with our values and legal obligations. Employees must understand and comply with this policy. Regular reviews will ensure it remains relevant with changing technology and security standards.\\n\\n4. Mobile Phone Policy\\n\\nOur Mobile Phone Policy defines standards for responsible use of mobile devices within the organization to ensure alignment with company values and legal requirements.\\n\\nAcceptable Use: Mobile devices are primarily for work-related tasks. Limited personal use is allowed if it does not disrupt work responsibilities.\\n\\nSecurity: Secure your mobile device and credentials. Be cautious with app downloads and links from unknown sources, and report any security issues promptly.\\n\\nConfidentiality: Avoid sharing sensitive company information via unsecured messaging apps or emails. Exercise caution when discussing company matters in public.\\n\\nCost Management: Personal use of mobile phones should be separate from company accounts, and any personal charges on company-issued phones must be reimbursed.\\n\\nCompliance: Comply with all relevant laws and regulations concerning mobile phone usage, including data protection and privacy laws.\\n\\nLost or Stolen Devices: Immediately report any lost or stolen mobile devices to the IT department or your supervisor.\\n\\nConsequences: Non-compliance with this policy may result in disciplinary actions, including potential loss of mobile phone privileges.\\n\\nThis policy encourages the responsible use of mobile devices in line with legal and ethical standards. Employees are expected to understand and follow these guidelines. The policy is regularly reviewed to stay current with evolving technology and security best practices.\\n\")]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `pprint` function to print the first 1000 characters of the `page_content` here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1. Code of Conduct\\n'\n",
      " '\\n'\n",
      " 'Our Code of Conduct establishes the core values and ethical standards that '\n",
      " 'all members of our organization must adhere to. We are committed to '\n",
      " 'fostering a workplace characterized by integrity, respect, and '\n",
      " 'accountability.\\n'\n",
      " '\\n'\n",
      " 'Integrity: We commit to the highest ethical standards by being honest and '\n",
      " 'transparent in all our dealings, whether with colleagues, clients, or the '\n",
      " 'community. We protect sensitive information and avoid conflicts of '\n",
      " 'interest.\\n'\n",
      " '\\n'\n",
      " \"Respect: We value diversity and every individual's contribution. \"\n",
      " 'Discrimination, harassment, or any form of disrespect is not tolerated. We '\n",
      " 'promote an inclusive environment where differences are respected, and '\n",
      " 'everyone is treated with dignity.\\n'\n",
      " '\\n'\n",
      " 'Accountability: We are responsible for our actions and decisions, complying '\n",
      " 'with all relevant laws and regulations. We aim for continuous improvement '\n",
      " 'and report any breaches of this code, supporting investigations into such '\n",
      " 'matters.\\n'\n",
      " '\\n'\n",
      " 'Safety: We prioritize the safety of our employees, c')\n"
     ]
    }
   ],
   "source": [
    "pprint(data[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from PDF files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we may have files in PDF format that we want to load for processing.\n",
    "\n",
    "LangChain provides several classes for loading PDFs. Here, we introduce two classes: `PyPDFLoader` and `PyMuPDFLoader`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyPDFLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the PDF using `PyPDFLoader` into an array of documents, where each document contains the page content and metadata with the page number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(pdf_url)\n",
    "\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first page of the PDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='LAB: L ARGE -SCALE ALIGNMENT FOR CHATBOTS\n",
      "MIT-IBM Watson AI Lab and IBM Research\n",
      "Shivchander Sudalairaj∗\n",
      "Abhishek Bhandwaldar∗\n",
      "Aldo Pareja∗\n",
      "Kai Xu\n",
      "David D. Cox\n",
      "Akash Srivastava∗,†\n",
      "*Equal Contribution, †Corresponding Author\n",
      "ABSTRACT\n",
      "This work introduces LAB (Large-scale Alignment for chatBots), a novel method-\n",
      "ology designed to overcome the scalability challenges in the instruction-tuning\n",
      "phase of large language model (LLM) training. Leveraging a taxonomy-guided\n",
      "synthetic data generation process and a multi-phase tuning framework, LAB sig-\n",
      "nificantly reduces reliance on expensive human annotations and proprietary mod-\n",
      "els like GPT-4. We demonstrate that LAB-trained models can achieve compet-\n",
      "itive performance across several benchmarks compared to models trained with\n",
      "traditional human-annotated or GPT-4 generated synthetic data. Thus offering a\n",
      "scalable, cost-effective solution for enhancing LLM capabilities and instruction-\n",
      "following behaviors without the drawbacks of catastrophic forgetting, marking a\n",
      "step forward in the efficient training of LLMs for a wide range of applications.\n",
      "1 I NTRODUCTION\n",
      "Large language models (LLMs) have achieved remarkable levels of success in various natural lan-\n",
      "guage processing (NLP) applications, including question-answering , entity extraction , and sum-\n",
      "marization . This has been made possible, in large part, by the introduction of the transformer\n",
      "architecture , which can leverage large amounts of unlabeled, unstructured data, enabling the scal-\n",
      "ing of LLMs to billions, or even trillions of parameters. LLMs are typically trained in phases: a\n",
      "self-supervised pre-training phase, followed by supervised alignment tuning phases.\n",
      "The majority of the cost of training an LLM comes from the pre-training phase. During this phase, a\n",
      "model is trained in an auto-regressive manner to predict the next token in the target language using\n",
      "trillions of tokens worth of unlabeled data, requiring thousands of GPUs training for months at a\n",
      "time. Alignment tuning, typically happens in two stages: instruction tuning, followed by prefer-\n",
      "ence tuning. Instruction tuning is more akin to the traditional model training approach in machine\n",
      "learning, where the model is trained directly on tasks of interest. In this stage, the model is given a\n",
      "task description in the form of an natural language instuction (e.g. Summarize the following news\n",
      "article in 2 lines: {News article }) and the model is trained to maximize the likelihood of the pro-\n",
      "vided ground truth summary. Preference tuning, on the other hand, is done using techniques such\n",
      "as RLHF (Stiennon et al., 2022; Ouyang et al., 2022) and DPO (Rafailov et al., 2023), where the\n",
      "response from an instruction-tuned model is rated as preferred or unpreferred using human feedback.\n",
      "In comparison to pre-training, the instruction tuning and preference tuning stages comprise a small\n",
      "fraction of the overall training procedure, both in terms of the data used as well as the compute\n",
      "infrastructure required to train models Touvron et al. (2023). For example, Meta’s LLaMA 2 models\n",
      "were trained with just tens of thousands of high quality human-generated instruction/response data\n",
      "pairs, followed by multiple rounds of RLHF with a comparatively limited number of examples as\n",
      "compared to pretraining data volumes Touvron et al. (2023). From a traditional machine learning\n",
      "training perspective, this imbalance in the scale across the phases is unconventional—typically one\n",
      "would expect a model to perform best when it has been trained directly on the desired tasks, using as\n",
      "much data as possible. The deviation from the tradtional LLM approach relies on the idea that pre-\n",
      "1arXiv:2403.01081v3  [cs.CL]  29 Apr 2024' metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(pages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first three pages of the PDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page number 1\n",
      "page_content='LAB: L ARGE -SCALE ALIGNMENT FOR CHATBOTS\n",
      "MIT-IBM Watson AI Lab and IBM Research\n",
      "Shivchander Sudalairaj∗\n",
      "Abhishek Bhandwaldar∗\n",
      "Aldo Pareja∗\n",
      "Kai Xu\n",
      "David D. Cox\n",
      "Akash Srivastava∗,†\n",
      "*Equal Contribution, †Corresponding Author\n",
      "ABSTRACT\n",
      "This work introduces LAB (Large-scale Alignment for chatBots), a novel method-\n",
      "ology designed to overcome the scalability challenges in the instruction-tuning\n",
      "phase of large language model (LLM) training. Leveraging a taxonomy-guided\n",
      "synthetic data generation process and a multi-phase tuning framework, LAB sig-\n",
      "nificantly reduces reliance on expensive human annotations and proprietary mod-\n",
      "els like GPT-4. We demonstrate that LAB-trained models can achieve compet-\n",
      "itive performance across several benchmarks compared to models trained with\n",
      "traditional human-annotated or GPT-4 generated synthetic data. Thus offering a\n",
      "scalable, cost-effective solution for enhancing LLM capabilities and instruction-\n",
      "following behaviors without the drawbacks of catastrophic forgetting, marking a\n",
      "step forward in the efficient training of LLMs for a wide range of applications.\n",
      "1 I NTRODUCTION\n",
      "Large language models (LLMs) have achieved remarkable levels of success in various natural lan-\n",
      "guage processing (NLP) applications, including question-answering , entity extraction , and sum-\n",
      "marization . This has been made possible, in large part, by the introduction of the transformer\n",
      "architecture , which can leverage large amounts of unlabeled, unstructured data, enabling the scal-\n",
      "ing of LLMs to billions, or even trillions of parameters. LLMs are typically trained in phases: a\n",
      "self-supervised pre-training phase, followed by supervised alignment tuning phases.\n",
      "The majority of the cost of training an LLM comes from the pre-training phase. During this phase, a\n",
      "model is trained in an auto-regressive manner to predict the next token in the target language using\n",
      "trillions of tokens worth of unlabeled data, requiring thousands of GPUs training for months at a\n",
      "time. Alignment tuning, typically happens in two stages: instruction tuning, followed by prefer-\n",
      "ence tuning. Instruction tuning is more akin to the traditional model training approach in machine\n",
      "learning, where the model is trained directly on tasks of interest. In this stage, the model is given a\n",
      "task description in the form of an natural language instuction (e.g. Summarize the following news\n",
      "article in 2 lines: {News article }) and the model is trained to maximize the likelihood of the pro-\n",
      "vided ground truth summary. Preference tuning, on the other hand, is done using techniques such\n",
      "as RLHF (Stiennon et al., 2022; Ouyang et al., 2022) and DPO (Rafailov et al., 2023), where the\n",
      "response from an instruction-tuned model is rated as preferred or unpreferred using human feedback.\n",
      "In comparison to pre-training, the instruction tuning and preference tuning stages comprise a small\n",
      "fraction of the overall training procedure, both in terms of the data used as well as the compute\n",
      "infrastructure required to train models Touvron et al. (2023). For example, Meta’s LLaMA 2 models\n",
      "were trained with just tens of thousands of high quality human-generated instruction/response data\n",
      "pairs, followed by multiple rounds of RLHF with a comparatively limited number of examples as\n",
      "compared to pretraining data volumes Touvron et al. (2023). From a traditional machine learning\n",
      "training perspective, this imbalance in the scale across the phases is unconventional—typically one\n",
      "would expect a model to perform best when it has been trained directly on the desired tasks, using as\n",
      "much data as possible. The deviation from the tradtional LLM approach relies on the idea that pre-\n",
      "1arXiv:2403.01081v3  [cs.CL]  29 Apr 2024' metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 0}\n",
      "page number 2\n",
      "page_content='training captures enough of the distribution of language and knowledge, such that a small amount of\n",
      "supervised training can “unlock” or shape latent abilities related to the ultimate desired instruction-\n",
      "following behavior of the model. However, unlike the unstructured data that is abundantly available\n",
      "in the public domain, high-quality, human-generated task-specific instruction data is costly to pro-\n",
      "cure, even via crowd-sourcing, and human-generated instruction data is typically closely guarded\n",
      "by model builders, even for ostensibly “open” model-building efforts. In this work, we address\n",
      "the challenges associated with scaling of the alignment-tuning phase and propose a new method\n",
      "called LAB: Large-scale Alignment for chatBots. The LAB method consists of two components:\n",
      "(i) a taxonomy-guided synthetic data generation method and quality assurance process that yields a\n",
      "highly diverse and high-quality instruction dataset, without resorting to the use of proprietary LLMs\n",
      "like GPT-4 or substantial human curation, and (ii) a novel multi-phase training framework and un-\n",
      "conventional tuning regime that allows for adding new knowledge and instruction-following abilities\n",
      "into pre-trained LLMs without suffering from catastrophic forgetting. Our findings show that LAB-\n",
      "trained models can perform competitively with proprietary and open-source models that use human\n",
      "annotations and/or synthetic data generated using GPT-4 on a number of benchmarks.\n",
      "2 R ELATED WORK\n",
      "Existing methods for instruction tuning typically either rely on humans for generating high-quality\n",
      "datasets, or use synthetic data generation using a large teacher model. OpenAI (Ouyang et al.,\n",
      "2022) arguably set the standard for model alignment from human data, employing human annota-\n",
      "tors to gather data for supervised fine tuning (SFT) and reinforcement learning with human feed-\n",
      "back (RLHF) training. Collecting human-generated data for these steps is complex undertaking; the\n",
      "selection of annotators requires a rigorous multi-stage screening process aimed at achieving high\n",
      "inter-annotator agreement, and collecting even modest amounts data (by LLM standards) requires\n",
      "the coordination of large groups of annotators. The creators of the LLaMA 2 model series (Touvron\n",
      "et al., 2023) followed a similar recipe, collecting tens of thousands of human-generated instruction\n",
      "samples, and approximately 1 million human-annotated binary comparisons for reward modeling.\n",
      "Not only are such approaches expensive and time consuming, but they can also potentially limit\n",
      "agility in exploring the space of instructions and capabilities the model is trained to perform. Alter-\n",
      "natives to this approach, such as transforming existing human datasets into instructions via templat-\n",
      "ing (Wei et al.) can be more cost effective, but face limitations in the naturalness and length of the\n",
      "responses used for training.\n",
      "More recently, training with synthetic data generated from LLMs has emerged as an alternative to\n",
      "purely human-data-based approaches. Wang et al. (2023) introduced Self-Instruct, which leverages\n",
      "a small number of handwritten human seed instructions as input to bootstrapping process to generate\n",
      "a large number of samples using an LLM’s own generation abilities. Taori et al. (2023) built upon\n",
      "Self-Instruct, using a larger teacher model to generate synthetic data to train a smaller student model,\n",
      "and incorporating principles in the generation prompt to promote diversity in the generated instruc-\n",
      "tion data. Xu et al. (2023) introduces Evol-Instruct, another variant of Self-Instruct, that synthesizes\n",
      "iteratively more complex instruction to overcome shortcomings of previous methods. Mukherjee\n",
      "et al. (2023), Mitra et al. (2023) present a synthetic data generation approach to enhance task di-\n",
      "versity and scalability, alongside a progressive training framework aimed at improving the model’s\n",
      "reasoning ability and response style to match teacher models. This is achieved by generating rich' metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 1}\n",
      "page number 3\n",
      "page_content='versity and scalability, alongside a progressive training framework aimed at improving the model’s\n",
      "reasoning ability and response style to match teacher models. This is achieved by generating rich\n",
      "reasoning signals in the generated answer and progressively training on datasets of varying difficulty\n",
      "in incremental phases.\n",
      "Similar to LAB, concurrent work, GLAN (Li et al., 2024), employs a semi-automatic approach to\n",
      "synthetic data generation that uses a human-curated taxonomy to generate instruction tuning data\n",
      "from a teacher model. However, as explained in section 3.2.2, unlike LAB, GLAN cannot be used\n",
      "to generate synthetic data from domains that are not captured in the teacher model’s support. As\n",
      "such, while LAB uses the open-source Mixtral model as the teacher, like many other synthetic\n",
      "data generation approaches, GLAN has to rely on a large proprietary model (GPT-4). This poses\n",
      "complicated questions about the usability of generated data (especially for commercial purposes)\n",
      "since the terms of use of proprietary models typically forbid using the model to improve other\n",
      "models.\n",
      "2' metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 1}\n"
     ]
    }
   ],
   "source": [
    "for p,page in enumerate(pages[0:3]):\n",
    "    print(f\"page number {p+1}\")\n",
    "    print(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyMuPDFLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PyMuPDFLoader` is the fastest of the PDF parsing options. It provides detailed metadata about the PDF and its pages, and returns one document per page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.pdf.PyMuPDFLoader at 0x7f47618681a0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyMuPDFLoader(pdf_url)\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='LAB: LARGE-SCALE ALIGNMENT FOR CHATBOTS\n",
      "MIT-IBM Watson AI Lab and IBM Research\n",
      "Shivchander Sudalairaj∗\n",
      "Abhishek Bhandwaldar∗\n",
      "Aldo Pareja∗\n",
      "Kai Xu\n",
      "David D. Cox\n",
      "Akash Srivastava∗,†\n",
      "*Equal Contribution, †Corresponding Author\n",
      "ABSTRACT\n",
      "This work introduces LAB (Large-scale Alignment for chatBots), a novel method-\n",
      "ology designed to overcome the scalability challenges in the instruction-tuning\n",
      "phase of large language model (LLM) training. Leveraging a taxonomy-guided\n",
      "synthetic data generation process and a multi-phase tuning framework, LAB sig-\n",
      "nificantly reduces reliance on expensive human annotations and proprietary mod-\n",
      "els like GPT-4. We demonstrate that LAB-trained models can achieve compet-\n",
      "itive performance across several benchmarks compared to models trained with\n",
      "traditional human-annotated or GPT-4 generated synthetic data. Thus offering a\n",
      "scalable, cost-effective solution for enhancing LLM capabilities and instruction-\n",
      "following behaviors without the drawbacks of catastrophic forgetting, marking a\n",
      "step forward in the efficient training of LLMs for a wide range of applications.\n",
      "1\n",
      "INTRODUCTION\n",
      "Large language models (LLMs) have achieved remarkable levels of success in various natural lan-\n",
      "guage processing (NLP) applications, including question-answering , entity extraction , and sum-\n",
      "marization . This has been made possible, in large part, by the introduction of the transformer\n",
      "architecture , which can leverage large amounts of unlabeled, unstructured data, enabling the scal-\n",
      "ing of LLMs to billions, or even trillions of parameters. LLMs are typically trained in phases: a\n",
      "self-supervised pre-training phase, followed by supervised alignment tuning phases.\n",
      "The majority of the cost of training an LLM comes from the pre-training phase. During this phase, a\n",
      "model is trained in an auto-regressive manner to predict the next token in the target language using\n",
      "trillions of tokens worth of unlabeled data, requiring thousands of GPUs training for months at a\n",
      "time. Alignment tuning, typically happens in two stages: instruction tuning, followed by prefer-\n",
      "ence tuning. Instruction tuning is more akin to the traditional model training approach in machine\n",
      "learning, where the model is trained directly on tasks of interest. In this stage, the model is given a\n",
      "task description in the form of an natural language instuction (e.g. Summarize the following news\n",
      "article in 2 lines: {News article}) and the model is trained to maximize the likelihood of the pro-\n",
      "vided ground truth summary. Preference tuning, on the other hand, is done using techniques such\n",
      "as RLHF (Stiennon et al., 2022; Ouyang et al., 2022) and DPO (Rafailov et al., 2023), where the\n",
      "response from an instruction-tuned model is rated as preferred or unpreferred using human feedback.\n",
      "In comparison to pre-training, the instruction tuning and preference tuning stages comprise a small\n",
      "fraction of the overall training procedure, both in terms of the data used as well as the compute\n",
      "infrastructure required to train models Touvron et al. (2023). For example, Meta’s LLaMA 2 models\n",
      "were trained with just tens of thousands of high quality human-generated instruction/response data\n",
      "pairs, followed by multiple rounds of RLHF with a comparatively limited number of examples as\n",
      "compared to pretraining data volumes Touvron et al. (2023). From a traditional machine learning\n",
      "training perspective, this imbalance in the scale across the phases is unconventional—typically one\n",
      "would expect a model to perform best when it has been trained directly on the desired tasks, using as\n",
      "much data as possible. The deviation from the tradtional LLM approach relies on the idea that pre-\n",
      "1\n",
      "arXiv:2403.01081v3  [cs.CL]  29 Apr 2024\n",
      "' metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'file_path': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 0, 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20240501000524Z', 'modDate': 'D:20240501000524Z', 'trapped': ''}\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `metadata` attribute reveals that `PyMuPDFLoader` provides more detailed metadata information than `PyPDFLoader`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from Markdown files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, our file source might be in Markdown format.\n",
    "\n",
    "LangChain provides the `UnstructuredMarkdownLoader` to load content from Markdown files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-22 16:38:35--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/eMSP5vJjj9yOfAacLZRWsg/markdown-sample.md\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "200 OKequest sent, awaiting response... \n",
      "Length: 3398 (3.3K) [text/markdown]\n",
      "Saving to: ‘markdown-sample.md’\n",
      "\n",
      "markdown-sample.md  100%[===================>]   3.32K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-05-22 16:38:35 (494 MB/s) - ‘markdown-sample.md’ saved [3398/3398]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/eMSP5vJjj9yOfAacLZRWsg/markdown-sample.md'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.markdown.UnstructuredMarkdownLoader at 0x7f477c3203e0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_path = \"markdown-sample.md\"\n",
    "loader = UnstructuredMarkdownLoader(markdown_path)\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'markdown-sample.md'}, page_content='An h1 header\\n\\nParagraphs are separated by a blank line.\\n\\n2nd paragraph. Italic, bold, and monospace. Itemized lists\\nlook like:\\n\\nthis one\\n\\nthat one\\n\\nthe other one\\n\\nNote that --- not considering the asterisk --- the actual text\\ncontent starts at 4-columns in.\\n\\nBlock quotes are\\nwritten like so.\\n\\nThey can span multiple paragraphs,\\nif you like.\\n\\nUse 3 dashes for an em-dash. Use 2 dashes for ranges (ex., \"it\\'s all\\nin chapters 12--14\"). Three dots ... will be converted to an ellipsis.\\nUnicode is supported. ☺\\n\\nAn h2 header\\n\\nHere\\'s a numbered list:\\n\\nfirst item\\n\\nsecond item\\n\\nthird item\\n\\nNote again how the actual text starts at 4 columns in (4 characters\\nfrom the left side). Here\\'s a code sample:\\n\\nAs you probably guessed, indented 4 spaces. By the way, instead of\\nindenting the block, you can use delimited blocks, if you like:\\n\\n~~~\\ndefine foobar() {\\n    print \"Welcome to flavor country!\";\\n}\\n~~~\\n\\n(which makes copying & pasting easier). You can optionally mark the\\ndelimited block for Pandoc to syntax highlight it:\\n\\n~~~python\\nimport time\\n\\nQuick, count to ten!\\n\\nfor i in range(10):\\n    # (but not too quick)\\n    time.sleep(0.5)\\n    print i\\n~~~\\n\\nAn h3 header\\n\\nNow a nested list:\\n\\nFirst, get these ingredients:\\n\\ncarrots\\ncelery\\nlentils\\n\\nBoil some water.\\n\\nDump everything in the pot and follow\\n    this algorithm:\\nfind wooden spoon\\nuncover pot\\nstir\\ncover pot\\nbalance wooden spoon precariously on pot handle\\nwait 10 minutes\\ngoto first step (or shut off burner when done)\\n\\nDo not bump wooden spoon or it will fall.\\n\\nNotice again how text always lines up on 4-space indents (including\\nthat last line which continues item 3 above).\\n\\nHere\\'s a link to a website, to a local\\ndoc, and to a section heading in the current\\ndoc. Here\\'s a footnote [^1].\\n\\n[^1]: Footnote text goes here.\\n\\nTables can look like this:\\n\\nsize  material      color\\n\\n9     leather       brown\\n10    hemp canvas   natural\\n11    glass         transparent\\n\\nTable: Shoes, their sizes, and what they\\'re made of\\n\\n(The above is the caption for the table.) Pandoc also supports\\nmulti-line tables:\\n\\nkeyword   text\\n\\nred       Sunsets, apples, and\\n          other red or reddish\\n          things.\\n\\ngreen     Leaves, grass, frogs\\n          and other things it\\'s\\n          not easy being.\\n\\nA horizontal rule follows.\\n\\nHere\\'s a definition list:\\n\\napples\\n  : Good for making applesauce.\\noranges\\n  : Citrus!\\ntomatoes\\n  : There\\'s no \"e\" in tomatoe.\\n\\nAgain, text is indented 4 spaces. (Put a blank line between each\\nterm/definition pair to spread things out more.)\\n\\nHere\\'s a \"line block\":\\n\\n| Line one\\n|   Line too\\n| Line tree\\n\\nand images can be specified like so:\\n\\nInline math equations go in like so: $\\\\omega = d\\\\phi / dt$. Display\\nmath should get its own line and be put in in double-dollarsigns:\\n\\n$$I = \\\\int \\\\rho R^{2} dV$$\\n\\nAnd note that you can backslash-escape any punctuation characters\\nwhich you wish to be displayed literally, ex.: `foo`, *bar*, etc.')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from JSON files\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JSONLoader uses a specified [jq schema](https://en.wikipedia.org/wiki/Jq_(programming_language)) to parse the JSON files. It uses the jq python package, which we've installed before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-22 16:38:37--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hAmzVJeOUAMHzmhUHNdAUg/facebook-chat.json\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "200 OKequest sent, awaiting response... \n",
      "Length: 2167 (2.1K) [application/json]\n",
      "Saving to: ‘facebook-chat.json’\n",
      "\n",
      "facebook-chat.json  100%[===================>]   2.12K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-05-22 16:38:37 (420 MB/s) - ‘facebook-chat.json’ saved [2167/2167]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hAmzVJeOUAMHzmhUHNdAUg/facebook-chat.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's use `pprint` to take a look at the JSON file and its structure. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='facebook-chat.json'\n",
    "data = json.loads(Path(file_path).read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': {'creation_timestamp': 1675549016, 'uri': 'image_of_the_chat.jpg'},\n",
      " 'is_still_participant': True,\n",
      " 'joinable_mode': {'link': '', 'mode': 1},\n",
      " 'magic_words': [],\n",
      " 'messages': [{'content': 'Bye!',\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675597571851},\n",
      "              {'content': 'Oh no worries! Bye',\n",
      "               'sender_name': 'User 1',\n",
      "               'timestamp_ms': 1675597435669},\n",
      "              {'content': 'No Im sorry it was my mistake, the blue one is not '\n",
      "                          'for sale',\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675596277579},\n",
      "              {'content': 'I thought you were selling the blue one!',\n",
      "               'sender_name': 'User 1',\n",
      "               'timestamp_ms': 1675595140251},\n",
      "              {'content': 'Im not interested in this bag. Im interested in the '\n",
      "                          'blue one!',\n",
      "               'sender_name': 'User 1',\n",
      "               'timestamp_ms': 1675595109305},\n",
      "              {'content': 'Here is $129',\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675595068468},\n",
      "              {'photos': [{'creation_timestamp': 1675595059,\n",
      "                           'uri': 'url_of_some_picture.jpg'}],\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675595060730},\n",
      "              {'content': 'Online is at least $100',\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675595045152},\n",
      "              {'content': 'How much do you want?',\n",
      "               'sender_name': 'User 1',\n",
      "               'timestamp_ms': 1675594799696},\n",
      "              {'content': 'Goodmorning! $50 is too low.',\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675577876645},\n",
      "              {'content': 'Hi! Im interested in your bag. Im offering $50. Let '\n",
      "                          'me know if you are interested. Thanks!',\n",
      "               'sender_name': 'User 1',\n",
      "               'timestamp_ms': 1675549022673}],\n",
      " 'participants': [{'name': 'User 1'}, {'name': 'User 2'}],\n",
      " 'thread_path': 'inbox/User 1 and User 2 chat',\n",
      " 'title': 'User 1 and User 2 chat'}\n"
     ]
    }
   ],
   "source": [
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `JSONLoader` to load data from the JSON file. However, JSON files can have various attribute-value pairs. If we want to load a specific attribute and its value, we need to set an appropriate `jq schema`.\n",
    "\n",
    "So for example, if we want to load the `content` from the JSON file, we need to set `jq_schema='.messages[].content'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2331852911.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[21], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    data = loader.load()We use JSONLoader to load data from the JSON file. However, JSON files can have various attribute-value pairs. If we want to load a specific attribute and its value, we need to set an appropriate jq schema.\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "loader = JSONLoader(\n",
    "    file_path=file_path,\n",
    "    jq_schema='.messages[].content',\n",
    "    text_content=False)\n",
    "\n",
    "data = loader.load()We use JSONLoader to load data from the JSON file. However, JSON files can have various attribute-value pairs. If we want to load a specific attribute and its value, we need to set an appropriate jq schema.\n",
    "\n",
    "So for example, if we want to load the content from the JSON file, we need to set jq_schema='.messages[].content'.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from CSV files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV files are a common format for storing tabular data. The `CSVLoader` provides a convenient way to read and process this data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IygVG_j0M87BM4Z0zFsBMA/mlb-teams-2012.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path='mlb-teams-2012.csv')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you load data from a CSV file, the loader typically creates a separate `Document` object for each row of data in the CSV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UnstructuredCSVLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to `CSVLoader`, which treats each row as an individual document with headers defining the data, `UnstructuredCSVLoader` considers the entire CSV file as a single unstructured table element. This approach is beneficial when you want to analyze the data as a complete table rather than as separate entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredCSVLoader(\n",
    "    file_path=\"mlb-teams-2012.csv\", mode=\"elements\"\n",
    ")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0].metadata[\"text_as_html\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from URL/Website files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually we use `BeautifulSoup` package to load and parse a HTML or XML file. But it has some limitations.\n",
    "\n",
    "The following code is using `BeautifulSoup` to parse a website. Let's see what limitation it has.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.ibm.com/topics/langchain'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the print output, we can see that `BeautifulSoup` not only load the web content, but also a lot of HTML tags and external links, which are not necessary if we just want to load the text content of the web.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So LangChain's `WebBaseLoader` can effectively address this limitation.\n",
    "\n",
    "`WebBaseLoader` is designed to extract all text from HTML webpages and convert it into a document format suitable for further processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load from single web page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://www.ibm.com/topics/langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load from multiple web pages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load multiple webpages simultaneously by passing a list of URLs to the loader. This will return a list of documents corresponding to the order of the URLs provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader([\"https://www.ibm.com/topics/langchain\", \"https://www.redhat.com/en/topics/ai/what-is-instructlab\"])\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from WORD files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Docx2txtLoader` is utilized to convert Word documents into a document format suitable for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/94hiHUNLZdb0bLMkrCh79g/file-sample.docx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Docx2txtLoader(\"file-sample.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from Unstructured Files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we need to load content from various text sources and formats without writing a separate loader for each one. Additionally, when a new file format emerges, we want to save time by not having to write a new loader for it. `UnstructuredFileLoader` addresses this need by supporting the loading of multiple file types. Currently, `UnstructuredFileLoader` can handle text files, PowerPoints, HTML, PDFs, images, and more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can load `.txt` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredFileLoader(\"new-Policies.txt\")\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can load `.md` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredFileLoader(\"markdown-sample.md\")\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple files with different formats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even load a list of files with different formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"markdown-sample.md\", \"new-Policies.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredFileLoader(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Try to use other PDF loaders\n",
    "\n",
    "There are many other PDF loaders in LangChain, for example, `PyPDFium2Loader`. Can you use this PDF loader to load the PDF and see the difference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The actual length of the printed content is: 1000 characters.\n",
      "A Comprehensive Review of Low-Rank\n",
      "Adaptation in Large Language Models for\n",
      "Efficient Parameter Tuning\n",
      "September 10, 2024\n",
      "Abstract\n",
      "Natural Language Processing (NLP) often involves pre-training large\n",
      "models on extensive datasets and then adapting them for specific tasks\n",
      "through fine-tuning. However, as these models grow larger, like GPT-3\n",
      "with 175 billion parameters, fully fine-tuning them becomes computa\u0002tionally expensive. We propose a novel method called LoRA (Low-Rank\n",
      "Adaptation) that significantly reduces the overhead by freezing the orig\u0002inal model weights and only training small rank decomposition matrices.\n",
      "This leads to up to 10,000 times fewer trainable parameters and reduces\n",
      "GPU memory usage by three times. LoRA not only maintains but some\u0002times surpasses fine-tuning performance on models like RoBERTa, De\u0002BERTa, GPT-2, and GPT-3. Unlike other methods, LoRA introduces\n",
      "no extra latency during inference, making it more efficient for practical\n",
      "applications. All relevan\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "\n",
    "pdf_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/WgM1DaUn2SYPcCg_It57tA/A-Comprehensive-Review-of-Low-Rank-Adaptation-in-Large-Language-Models-for-Efficient-Parameter-Tuning-1.pdf\"\n",
    "\n",
    "loader = PyPDFium2Loader(pdf_url)\n",
    "data = loader.load()\n",
    "\n",
    "# Get the sliced content\n",
    "sliced_content = data[0].page_content[:1000]\n",
    "\n",
    "# Print the actual length of the sliced content\n",
    "print(f\"The actual length of the printed content is: {len(sliced_content)} characters.\")\n",
    "\n",
    "# Now print the content itself\n",
    "print(sliced_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "!pip install pypdfium2\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "\n",
    "loader = PyPDFium2Loader(pdf_url)\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Load from Arxiv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we have paper that we want to load from Arxiv, can you load this [paper](https://arxiv.org/abs/1605.08386) using `ArxivLoader`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Split Texts ---\n",
      "Chunk 1 (Length: 442):\n",
      "\\documentclass{article}\n",
      "   egin{document}\n",
      "    \\maketitle\n",
      "    \\section{Introduction}\n",
      "    Large language models (LLMs) are a type of machine learning model that can be trained on vast amounts of text data to generate human-like language. In recent years, LLMs have made significant advances in various natural language processing tasks, including language translation, text generation, and sentiment analysis.\n",
      "    \\subsection{History of LLMs}\n",
      "------------------------------\n",
      "\n",
      "Chunk 2 (Length: 410):\n",
      "\\subsection{History of LLMs}\n",
      "The earliest LLMs were developed in the 1980s and 1990s, but they were limited by the amount of data that could be processed and the computational power available at the time. In the past decade, however, advances in hardware and software have made it possible to train LLMs on massive datasets, leading to significant improvements in performance.\n",
      "\\subsection{Applications of LLMs}\n",
      "------------------------------\n",
      "\n",
      "Chunk 3 (Length: 261):\n",
      "\\subsection{Applications of LLMs}\n",
      "LLMs have many applications in the industry, including chatbots, content creation, and virtual assistants. They can also be used in academia for research in linguistics, psychology, and computational linguistics.\n",
      "\\end{document}\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# Note: You might need to install 'langchain' if not already installed.\n",
    "# !pip install --user \"langchain==0.2.11\"\n",
    "\n",
    "latex_text = \"\"\"\n",
    "    \\documentclass{article}\n",
    "    \\begin{document}\n",
    "    \\maketitle\n",
    "    \\section{Introduction}\n",
    "    Large language models (LLMs) are a type of machine learning model that can be trained on vast amounts of text data to generate human-like language. In recent years, LLMs have made significant advances in various natural language processing tasks, including language translation, text generation, and sentiment analysis.\n",
    "    \\subsection{History of LLMs}\n",
    "The earliest LLMs were developed in the 1980s and 1990s, but they were limited by the amount of data that could be processed and the computational power available at the time. In the past decade, however, advances in hardware and software have made it possible to train LLMs on massive datasets, leading to significant improvements in performance.\n",
    "\\subsection{Applications of LLMs}\n",
    "LLMs have many applications in the industry, including chatbots, content creation, and virtual assistants. They can also be used in academia for research in linguistics, psychology, and computational linguistics.\n",
    "\\end{document}\n",
    "\"\"\"\n",
    "\n",
    "# Define the text splitter for LaTeX\n",
    "# We define common LaTeX structural elements as separators\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # Example chunk size\n",
    "    chunk_overlap=50, # Example overlap\n",
    "    separators=[\n",
    "        \"\\n\\n\",       # Double newline (paragraph break)\n",
    "        \"\\n\",         # Single newline\n",
    "        \" \",          # Space\n",
    "        \"\",           # Character by character fallback\n",
    "        \"\\\\section{\", # LaTeX sections\n",
    "        \"\\\\subsection{\", # LaTeX subsections\n",
    "        \"\\\\subsubsection{\", # LaTeX subsubsections\n",
    "        \"\\\\chapter{\", # LaTeX chapters (if using a document class that supports them)\n",
    "        \"\\\\documentclass{\", # Document class\n",
    "        \"\\\\begin{document}\", # Document environment start\n",
    "        \"\\\\end{document}\", # Document environment end\n",
    "    ],\n",
    "    length_function=len,\n",
    "    is_separator_regex=False # Set to True if your separators are regex\n",
    ")\n",
    "\n",
    "# Split the LaTeX document\n",
    "texts = text_splitter.split_text(latex_text)\n",
    "\n",
    "# Print the resulting chunks\n",
    "print(\"--- Split Texts ---\")\n",
    "for i, chunk in enumerate(texts):\n",
    "    print(f\"Chunk {i+1} (Length: {len(chunk)}):\\n{chunk}\\n{'-'*30}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "    \n",
    "```python\n",
    "\n",
    "!pip install arxiv\n",
    "\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "docs = ArxivLoader(query=\"1605.08386\", load_max_docs=2).load()\n",
    "\n",
    "print(docs[0].page_content[:1000])\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kang Wang](https://www.linkedin.com/in/kangwang95/)\n",
    "\n",
    "Kang Wang is a Data Scientist in IBM. He is also a PhD Candidate in the University of Waterloo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Joseph Santarcangelo](https://www.linkedin.com/in/joseph-s-50398b136/)\n",
    "\n",
    "Joseph has a Ph.D. in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n",
    "\n",
    "[Hailey Quach](https://author.skills.network/instructors/hailey_quach)\n",
    "\n",
    "Hailey is a Data Scientist at IBM. She is also an undergraduate student at Concordia University, Montreal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "8ccd87848ab9e79d16c766e68c2292b6bf1eff17098bb52f22c15a7b9da59990"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
